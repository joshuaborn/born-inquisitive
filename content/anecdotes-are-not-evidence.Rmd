---
title: "Anecdotes Are Not Evidence"
draft: true
date: 2021-01-10
tags: ["fallacies"]
bibliography: "../bibliographies/anecdotes-are-not-evidence.bib"
csl: "../bibliographies/chicago-author-date.csl"
link-citations: true
implicit-figures: true
output:
  blogdown::html_page:
    md_extensions: ["+footnotes"]
    toc: true
    toc_depth: 5
include-before: |
  <p>The opening paragraph goes here.</p>
  <!--more-->
  ## Table of Contents
---

## Introduction

If enough time is spent around those that construe themselves as doing scientific inquiry, one is liable to encounter the maxim "anecdotes are not evidence." Anecdotes are stories that consist either of one's own personal experiences or the retelling of the personal experiences of others. The maxim "anecdotes are not evidence" is a healthy reminder that the use of anecdotes as evidence has numerous flaws.

However, the maxim "anecdotes are not evidence," like all slogans, is a short, catchy phrase and is devoid of any substantially informative content. While such a catch phrase can be used as a mnemonic device to recall deeper understanding, it can also be used without any cognizance of the issues to which it alludes. When used in this latter way, the maxim can be interpreted in ways that lead to implications that are false.

One such false implication that stems from an extremely literal interpretation of the maxim "anecdotes are not evidence" is that _nothing at all_ can be learned from anecdotes.

Such a literal interpretation can be used to undermine the maxim itself. For instance, one could note a case in which a serial sex offender was caught due to the testimony of victims, which is necessarily anecdotal, and conclude that those who subscribe to "anecdotes are not evidence" would throw out this testimony, setting the sex offender free. This is such a morally repugnant proposition for so many that it could be used as a way to discredit those who are fond of the maxim "anecdotes are not evidence."

This literal interpretation can also be used as a foolish obstacle to understanding. For instance, if one hears stories from friends that a certain road is congested with traffic during rush hour, one might dismiss these stories, citing "anecdotes are not evidence." One would have no one to blame but oneself when one finds oneself stuck in traffic on the very same road, waiting for the rush hour congestion to clear.

Surely, because of these implications, this overly literal interpretation does not capture what the maxim "anecdotes are not evidence" is meant to convey. This is a flaw not just with this particular maxim, but with slogans in general, which threaten to become [shibboleths](political-rhetoric-as-shibboleths.html) when used incognizantly in this way.

The purpose of this article is to articulate the issues in using anecdotes as evidence in a more substantial way than can be done with a pithy maxim. It begins by discussing the sort of inference for which anecdotes are entirely useless, and this is balanced with later discussion of valid uses of anecdotes.


## Misuses of Anecdotes

Anecdotal evidence is useless for two kinds of inference: inference about a larger population and inference about cause and effect. 

### Inference about a Larger Population
 
While the prevalence or magnitude of a variable among some number of individuals that can be directly observed might sometimes be of interest, interesting questions often pertain to a larger population than those that are directly observed. For instance, an epidemiologist is usually professionally interested in how many people contracted the seasonal flu in a given geographic area, not how many of the epidemiologist's friends contracted the seasonal flu. This is a question about a larger population, i.e., all the people in the given geographic area, that is based on a smaller, representative sample, e.g., surveys, hospital records, etc. Anecdotes shed no light on these kinds of questions.

### Inference about Cause and Effect

Practical questions often pertain to whether or not something causes an effect and the size of such an effect. For instance, in clinical trials, a potential therapy is given to volunteers in order to determine whether or not the therapy causes an improvement in a disease or an increase in survival time. It is not enough to know how many people receiving the treatment improve or to what extent they improve. Indeed, there are many diseases from which people will often recover of their own accord. Rather, the question that is at issue in a clinical trial is whether or not the potential therapy _caused_ any measured improvement and what the _size_ of this _effect_ was. Again, anecdotes are no help in answering this kind of question.


## Flaws of Anecdotes as Evidence

Flaws in using anecdotes as a kind of evidence can broadly be grouped into two categories: statistical and psychological. The statistical flaws are what specifically preclude anecdotes from being useful for inference about a larger population and for inference about cause and effect, but do not prevent anecdotes from being useful in other ways. The psychological flaws affect anecdotal evidence no matter what the use, and so these flaws must be acknowledged and accepted during the valid uses of anecdotes articulated later.

### Statistical Flaws

#### Insufficient Sample Size

Returning to the example of the seasonal flu, suppose one is interested in how effective vaccination was in preventing individuals from contracting the flu in the United States during the 2019-2020 flu season. Suppose there are 10 friends and family that one keeps in touch with regularly. Of these 10, 6 were vaccinated against the flu, and 1 caught the flu this year. The 1 who caught the flu was not vaccinated. What estimate should one make of how effective this year's vaccine was?

A rough estimate for the probability of catching the flu when not vaccinated could be 1 / 4 = 25%. But what about the probability that someone who was vaccinated caught the flu? None of one's friends and family that were vaccinated caught the flu this year, so continuing this method of rough estimation leads to an estimate of 0 / 6 = 0%.

Thus, one might be tempted to say that, given these observations, this year's flu vaccine was absolutely effective. However, this is mistaken. Flu vaccines are effective at decreasing the risk that a vaccinated individual has in catching the flu, but they do not decrease the risk to zero. [@noauthor_vaccine_2020]

The issue this example highlights is that anecdotal evidence rarely consists of enough observations to make an inference to a larger population or to make an inference about cause and effect. Given a sample size of only 10, it is more likely than not that there would _not be a single individual_ in the sample who both was vaccinated and who caught the flu.[^envelope] However, in a larger sample of, for instance, 1,000 individuals, there would be many individuals who were vaccinated and who caught the flu.[^envelope-estimate]

[^envelope]: This is based on some back-of-the-envelope math. The U. S. Census Bureau estimate the population of the United States to be approximately 328,000,000 people. [@noauthor_quickfacts_nodate] The proportion of people in the United States who received the flu vaccine during the 2019-2020 season is estimated to be 63.8% for all age groups. [@noauthor_flu_2020] Therefore, there were approximately 209,300,000 vaccinated individuals and 118,700,000 unvaccinated individuals in the United States during the 2019-2020 flu season.

     The Centers for Disease Control and Prevention estimates there were 38,000,000 flu illnesses in the United States during the 2019-2020 flu season. [@noauthor_estimated_2020]
     
     A rough estimate of the relative risk for flu vaccination of 50% is reasonable. [@noauthor_vaccine_2020]

     This results in estimated probabilities of 17.0% of catching the flu among the unvaccinated and 8.5% of catching the flu among the vaccinated.

     The probability a randomly selected individual both was vaccinated and caught the flu is therefore 63.8% &times; 8.5% &approx; 5.42%, and the probability a random sample of 10 individuals does not contain someone who both was vaccinated and caught the flu is (1 &minus; 5.42%)<sup>10</sup> &approx; 57.3%. 

[^envelope-estimate]: Using the back-of-the-envelope calculations above, there would be around 54 such individuals in a random sample of 1,000.

In this case, the anecdotal evidence of one's friends and family does not consist of a large enough number of observations to calculate even a crude estimate of vaccine efficacy.[^power]

Furthermore, note that any estimate of the probability of catching the flu based on these anecdotes would be prone to wild fluctuations. If another unvaccinated friend happened to catch the flu during the 2019-2020 season, the estimated probability of unvaccinated individuals catching the flu would jump from 25% to 2 / 4 = 50%. This small change in the sample would result in a large change in the estimate about the larger population, which is an indication that the inference is susceptible to being thrown off due to chance.

[^power]: This is a plausible example of the phenomenon that anecdotal observations often do not comprise a sufficient sample size for inference, one which does not require knowledge of statistical methods. For the more mathematically minded, this phenomenon can be explored further by way of power computations at various sample sizes, which can be found in relevant statistics texts.

#### Unrepresentative Samples

Perhaps there is an individual so popular that polling the individual's friends and family results in a large enough number of observations to make an inference about a larger population or about cause and effect. Even if this were the case, another issue remains. The sample created from such polling is not _representative_ of a larger population. Instead, the sample is heavily biased towards the idiosyncracies of the individual in question.

Returning to the example of flu vaccine efficacy in the United States during the 2019-2020 season, suppose that a particularly extroverted individual keeps in regular contact with not just 10, but 200 friends and family. However, suppose the extroverted individual caught the flu during the 2019-2020 season.

In this case, the sample might be large enough for estimation of vaccine efficacy, but this sample will be biased because the extroverted individual was in contact with the 200 other people in the sample and so exposed them to the flu virus. Therefore, the prevalence of flu illness in this sample is expected to be higher than in the general population, which includes individuals who had no direct exposure to the flu virus. Because of this, any estimations regarding the _size_ of the protective effect _caused_ by the vaccine will be thrown off.

This phenomenon is likely to occur whenever sampling is based on personal associations. Another example comes from attempts to infer the prevalence of political affiliations based on a particular individual's social network. Most voters in the United States are registered as either Republican or Democrat. Because human beings tend to conform to those around them and to seek out those with similar views, if one starts with a registered Republican, one will likely find a higher proportion of registered Republicans among that individual's friends and family. Likewise, if one starts with a registered Democrat, one will likely find a higher proportion of registered Democrats amongst that individual's friends and family. Neither of these samples will be representative of the _larger population_ of the United States.

##### The Fallacy of Celebrated Cases

This issue is even worse for anecdotes that come not from personal associations, but by way of the news media. Much of what qualifies a story as news is that it is rare, unusual, or shocking enough to be of interest. Thus, news stories collectively form an _intentionally_ unrepresentative sample of what is happening in a larger population. This phenomenon was illustrated dramatically by the media frenzy around shark attacks in 2001.

While shark attacks on humans can result in a personal tragedy of injury or death for those involved that should neither be minimized nor made light of, the media representation of shark attacks in 2001 was misleading and irresponsible. A particularly graphic shark attack case was covered on major news media outlets in August of 2001. This was soon followed by story after story of sharks attacking humans. _Time_ magazine ran a cover story labeling the summer of 2001 the "Summer of the Shark."

![
_The July 30, 2001 cover of_ Time _magazine, which labeled the summer of 2001 the "Summer of the Shark."_
](/images/summer-of-the-shark.jpg)

This created an impression that shark attacks were increasing at an alarming rate during 2001 and that shark attacks were a particularly prevalent risk. However, it was soon discovered that 2001 had only an average number of shark attacks; indeed, the number of shark attacks and number of fatalities from shark attacks was actually less in 2001 than in 2000. [@noauthor_summer_2002]  

The so-called "Summer of the Shark" was not a sudden and pronounced increase in shark attacks, but a sudden and pronounced increase in the coverage of shark attacks. However, shark attacks are infrequent compared with other risks humans expose themselves to on a daily basis, so this spike in news coverage encouraged unsubstantiated fears. Nonetheless, the sensational coverage of shark attacks during 2001 did not end until the September 11, 2001 terrorist attacks took over the news cycle. Since then, the so-called "Summer of the Shark" has become an object lesson in bad journalism. [@dempsey_summer_2016]

#### Summary

Using anecdotes as evidence is flawed in two ways pertaining to statistical inference: anecdotes form sample sizes that are often too small to make precise estimates and are prone to wild fluctuations, and anecdotes form unrepresentative samples that throw off any estimation based on them due to bias. For these reasons, anecdotes are not useful at all for conclusions about a larger population and for conclusions about cause and effect. 

In addition to these statistical concerns, there are other flaws in anecdotes as evidence pertaining to how human minds work.

### Psychological Flaws

Contrary to popular belief, human memory does not "work like a video camera, accurately recording the events we see and hear so that we can review and inspect them later," as 63% of respondents in a demographically representative sample of 1,500 people in the United States believed in a recent telephone poll. [@simons_what_2011] Instead, human memory is flawed. People often fail to remember true things about past experiences or believe they remember false things about past experiences. Furthermore, the social experience of retelling memories can influence how past events are remembered.

#### Flaws of Memory Recall

The psychological literature around the topic of memory is quite large, and a full review of what has been learned about the fallibility of memory recall is beyond the scope of this article. Instead, this article summarizes a few seminal experiments in the field in order to establish the core point that memory recall, according to the empirical evidence, is quite fallible.

[...]

#### Flaws of Serial Reproduction

[...]


## Uses of Anecdotes

### Investigation of Specific, Singular Incident

### Counterexample to Universal Assertion

### Precipitation of Further Investigation

### Illustrative Example of Phenomenon Supported by Other Evidence

## Conclusion


## Citations

::: {#refs}
:::


## Footnotes
