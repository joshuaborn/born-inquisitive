---
draft: true
title: "How to Scrutinize a Statistic"
date: 2021-12-01
tags: ["reasoning about evidence"]
bibliography: "../bibliographies/how-to-scrutinize-a-statistic.bib"
csl: "../bibliographies/chicago-author-date.csl"
link-citations: true
implicit-figures: true
output:
  blogdown::html_page:
    md_extensions: ["+footnotes"]
    toc: true
    toc_depth: 5
include-before: |
  <p>Opening paragraph goes here.</p>
  <!--more-->
  ## Table of Contents
---

```{r setup, echo=FALSE, message=FALSE}
```


## Introduction

There is a cautionary saying that warns to be wary of those who use statistics like a drunkard uses a lamppost &ndash; for support rather than for illumination.[^saying]

[^saying]: This saying was popularized in 1937, when it was commonly attributed to Scottish folklorist Andrew Lang. Andrew Lang died in 1912 and may or may not have originated it, as it does not appear in any of his extant writings. Regardless of the origin of this specific version of the saying, it was likely inspired by similar figures of speech which came before it. See [this article](https://quoteinvestigator.com/2014/01/15/stats-drunk/) at the Quote Investigator blog for more information about the origin of the saying.

![
A man literally using a lamppost for support, which is a metaphor used in a popular saying describing the misuse of statistics.<br/>
["Lamppost Man"](https://www.flickr.com/photos/78019868@N05/741397893)
by
[David Hodgson](https://www.flickr.com/photos/publicplaces/)
is licensed under
[CC BY 2.0](https://creativecommons.org/licenses/by/2.0/)
and is not modified.
](/images/lamppost-man.jpg)

The point of this saying is to warn about those who start with a preconceived idea of a conclusion that they want to reach and then search for statistics that appear to validate that preconceived conclusion.

This is a practice well known to statisticians, so much so that _How to Lie with Statistics_ [-@huff_how_1993] is still in print more than fifty years after it was originally published in 1954.

The practice is effective for at least three reasons.

First, statistics have some persuasive ability. They bring with them an atmosphere of scholarship, science, and quantitative certainty. Even when no relevant empirical observations have been made, citing statistics make it appear that a position is supported by empirical evidence.

Second, given enough time and effort, statistics can be manufactured that appear to support any arbitrary position. This can be out of intentional deception by bad actors. It can also be done accidentally by those who do not know how to do empirical inquiry well, by those afflicted by the psychology of motivated reasoning, or by researchers who out of desperation to further their academic careers in a "publish or perish" world, publish whatever they can.

![
A stressed young woman hunched over a laptop computer. Aspiring academicians are judged by the number of publications they have and the prestige of the journals in which the publications appear, while often competing for very few openings, a phenomenon called "publish or perish" colloquially.<br/>
[Image](https://pixabay.com/photos/laptop-woman-education-study-young-3087585/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3087585)
by
[Jan Va≈°ek](https://pixabay.com/users/jeshoots-com-264599/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3087585)
from
[Pixabay](https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3087585).
](/images/stressed-student.jpg)

Third, interpreting and scrutinizing statistics requires knowledge and skills that many laypeople do not possess. Indeed, those who have had education in statistics are sometimes prone to mistakes in using and interpreting statistical methods. Thus, misuses of statistics can go unnoticed, by the general public or by academic peer review. Indeed, as shall be seen, some misuses of statistics can actually become entrenched in academic practice.

This situation might seem so discouraging that you might be inclined to throw out the use of statistics altogether. However, this would be a rash mistake. Those who care about any social issues, medical issues, environmental issues, etc, are necessarily concerned about understanding phenomena that occur in large populations of individuals. Therefore, if you care about such issues, you are necessarily engaged in a statistical inquiry, whether you realizes this or not.

The solution for those who care about such issues, therefore, is to be competent at scrutinizing statistics. The alternative is to be preyed upon by charlatans or, perhaps worse, to become a charlatan yourself.

Another way to conceive of this is that _statistics should be questioned,_ always, by every person who encounters them and who must make a decision on whether or not to believe what is being reported.


### Purpose of This Article

The intent of this article is to assist the reader in the enterprise of questioning the statistics you encounter. It does not assume the reader has had any formal instruction in statistics, though such instruction would be valuable in understanding the concepts used throughout.

This article is not a replacement for courses in statistics. However, the converse is also true. Statistics courses are rarely taught from the standpoint of what questions are relevant to ask when encountering a statistic. Rather, introductory statistics courses usually focus on giving students a bag of statistical tools to use in their research activities.[^courses]

[^courses]: Statistics courses beyond the introductory level usually have another kind of content. Typically, introductory courses are labeled "applied statistics" and are concerned with how to use various statistical methods. Upper-level courses are usually termed "mathematical statistics" and are concerned with the mathematical foundations of results used in statistics and how the statistical techniques are thus derived.

There is merit to knowledge of this bag of tools even if you do not do empirical research in your own life. For instance, when the toxicity of something is reported, it will often be reported by the the median lethal dose (LD<sub>50</sub>) statistic. The LD<sub>50</sub> in turn is usually derived by way of logistic regression, which is one of the tools in the bag of tools typically covered in a two semester introductory statistics sequence at the university level. Therefore, if you want to understand where this statistic comes from, you need to understand logistic regression.

This article, however, instead of discussing any one specific statistical technique, discusses questions that should be asked of all statistics regardless of how they are derived. Thus, the content of this article is both different from and complementary to the content of statistics courses.

Examples of real world statistics scrutinized in the manner described in this article are not included in the body of this article. Instead, this article is intended to quickly summarize the method used for other articles of this blog that review the evidence pertaining to a specific factual question. Those articles refer back to the questions introduced here and constitute the real world examples for this article.


## Provenance

The first question to ask of any statistic is in regard to its _provenance._ The word "provenance" is borrowed from historians of art who use it to refer to the chronology of place and custody of a work of art traced back to its origin, though the word is now used by a variety of fields.

![
Three paintings on display in the National Gallery of Art in the United States. The practice of investigating the provenance of works of art has led to the word "provenance" being used in other contexts.<br/>
["Art Gallery"](https://www.flickr.com/photos/55267995@N04/14288414720)
by
[cjt71081](https://www.flickr.com/photos/55267995@N04/)
is licensed under
[CC BY-SA 2.0](https://creativecommons.org/licenses/by-sa/2.0/)
and is not modified.
](/images/national-gallery-of-art.jpg)

In the context of scrutinizing a statistic, determining its provenance consists of asking questions such as _"Where does this statistic come from?" "Who made the observations from which this statistic was derived?"_ and _"How were those observations made?"_

The reason this is the first question to ask of any statistic is that its answer is a prerequisite for subsequent questions to ask when scrutinizing a statistic. If you do not know where a statistic came from or how it was derived, you cannot scrutinize further.

One good thing about establishing the provenance of a statistic versus other kinds of provenance is that many if not most statistics of any quality are reported in a scholarly paper of some kind. Thus, the work of establishing provenance of a statistic usually ends with a very definitive result: a document that has a Digital Object Identifier (DOI) and that can be succinctly referenced in any one of a number of standardized citation formats.

This first question is phrased as "establishing provenance" and not simply "looking it up" because there are many situations where the mere act of determining where a statistic comes from is itself a fair amount of work.


### Lack of a Source

The most obvious situation that makes establishing provenance challenging is when a statistic is stated, but no source is referenced.

![
A sign that reads "citation needed" at a rally in Washington, DC, alluding to the tag used in Wikipedia for assertions made without any source given.<br/>
["Citation needed"](https://www.flickr.com/photos/87913776@N00/5129607997)
by
[futureatlas.com](https://www.flickr.com/photos/87913776@N00/)
is licensed under
[CC BY 2.0](https://creativecommons.org/licenses/by/2.0/)
and is not modified.
](/images/citation-needed.jpg)

This can occur in a variety of settings, such as when reading an article or when engaging in face-to-face conversation. If the medium allows for two-way communication, it might be prudent to ask the person stating the statistic where that person encountered the statistic.

If no source is provided, that immediately casts doubt on the standards of the person stating the statistic. Such a person has not done the due diligence of scrutinizing a statistic, but is still passing it along, which is a practice that can spread misinformation.

However, while the lack of a source casts doubt on the veracity of the person asserting a statistic, it does not provide insight on the veracity of the statistic itself. In these cases, the best you can do is withhold any judgment, either assenting or denying, and try to find the origin of the statistic yourself.

Today, with the plethora of Internet search tools available, the task of finding an origin is not impossible. It is simply a value judgment you must make as to whether you want to spend your time in this manner. It is certainly within the realm of reason to remain in a state of suspended judgment if your time is better spent on other matters.


### Misrepresented Source

When a source is provided, the obvious first step in establishing provenance is to check this source. However, sometimes this immediately becomes a dead end because the cited source does not actually assert the statistic in question.  This phenomenon can have a variety of causes.

For one, the person falsely citing a source to be the origin of a statistic might simply be making a mistake in good faith. Perhaps the person [remembered something incorrectly](/anecdotes-are-not-evidence.html#flaws-of-memory) or confused this source with another.

Alternatively, some people engage in this practice intentionally as an act of deception. The mere presence of a falsely cited source can lend a statistic an air of credibility it would not have otherwise because many people do not check sources.

Finally, there are whole branches of thought that do not believe they need empirical evidence. Thus, a cited source might be asserting something similar to a statistic, but if the source in question is based on ideology instead of empirical evidence, then this pseudo-statistic is just made-up. However, a gullible audience might treat it as a source.

In the first case of a simple mistake, you are left in the same state as you would have been if no source was provided.

In the second case of intentional deception or the third case of an ideological origin, you have strong evidence that the statistic in question was just made-up and can be dismissed. There is a subtlety here to be concerned with, nonetheless. Even if a particular made-up statistic is false, that tells you nothing about what a true statistic would be. For instance, if someone makes up a statistic that 80\% of the _cagot_ ethnic group live in poverty, and you find out this is just made-up, you still do not know what the actual poverty rate among the the _cagot_ is.[^cagot]

[^cagot]: The _cagot_ were a real ethnic group who lived in a region part of modern day Spain and France. People categorized as _"cagot"_ were the object of much prejudice, and their origins have remained a mystery to scholarship over the centuries.

Unfortunately, when either unsourced statistics or statistics with a misrepresented source are encountered, they leave you not having learned any new information. The exercise of encountering them is not entirely a waste of your time, however, if you take the view that they are opportunities to develop your skills at scrutinizing statistics.


### Chain of References

Sometimes you find that the source cited for a statistic, when checked, itself cites another source for the statistic in question. That source might then cite some other source, and that source might yet again cite another source.

Especially whenever Internet journalism churns out numerous shallow articles, this chain of references can get quite long, and this phenomenon can make the work of establishing provenance needlessly time consuming and tedious. However, all that is important in this process is identifying the origin of the statistic. The chain of references themselves neither add nor detract from the credibility of the statistic. Indeed, these chains of references could be avoided if anyone contributing to the chain did their due diligence by establishing provenance and including a reference to the original source.

This highlights yet another fallacy that is sometimes used by those who use statistics for support rather than illumination. You should not be impressed when a single statistic has numerous citations included with it. If anything, seeing multiple sources cited for the same statistic should raise your suspicion.

For one, citing multiple supposed sources is easily accomplished by citing every step in a chain of references to a single origin as if they were a separate source. If they all ultimately lead to the same origin, then there is no point in this multiplicity of citations, and using multiple citations like this is deceptive.

Secondly, for multiple citations that lead to different origins, representing them as if they have the same conclusion is a indication that the sources are being misrepresented. It is highly unlikely that several studies would arrive at the exact same statistic, since statistical inference involves some amount of sampling error. In these cases, either the error is not being reported, results are being treated as if they were equivalent when they are not,[^effect] or tactics of misrepresentation are being used.

[^effect]: Reporting results as if they were equivalent when they are not can be done using generic comparisons, which are discussed briefly in the section on effect size in this article and are a deep enough topic that they are the subject of their own article.


### Primary versus Secondary Sources

Encyclopedias such as Wikipedia can be helpful when establishing provenance, but with one major caveat. Establishing the provenance of a statistic as discussed here is tracing the statistic back to its _origin,_ which is a _primary source._  Wikipedia has a different mission than establishing provenance, and prefers _secondary sources,_ as can be read its documentation:

> Wikipedia articles should be based on reliable, published secondary sources and, to a lesser extent, on tertiary sources and primary sources. . . . A secondary source provides an author's own thinking based on primary sources, generally at least one step removed from an event. It contains an author's _analysis, evaluation, interpretation, or synthesis_ of the facts, evidence, concepts, and ideas taken from primary sources. [@noauthor_wikipediano_2021]

In this article's context, a secondary source does the work of scrutinizing a statistic, which is what you the reader are supposed to be doing. A secondary source might be useful for comparison, but is not a replacement for finding the primary source.

That being understood, Wikipedia does contain a lot of references to primary sources, as well, even though its "no original research" policy discourages their use except under limited circumstances. Furthermore, a good secondary source will reference the primary source, and so might be a useful step along a chain of references in order to establish provenance.


## Scope of Inference

Once the origin of a statistic is found, the next two genres of questions to ask about the statistic are both related to what is termed "scope of inference" in statistics courses. Scope of inference is divided into two concerns: whether or not the results of a study generalize to a larger population, and whether or not conclusions can be drawn about cause and effect. The former is a concern about sampling, and the latter is a concern about experimental assignment. Both of these concerns stem from the possibility of confounding variables, and both concerns are addressed by a form of randomization.


### Sampling

Sometimes a statistic is intended to summarize information about the state of affairs in a large population. For instance, a country might be interested in the poverty rate of its citizens, which can constitute a population of millions of individual people. If the statistic is intended to generalize, you should ask questions such as _"What population was sampled?"_ and _"How was the sampling done?"_

One approach to infer information about a large population is to take a representative sample from a population and use measurements of the sample to infer estimates about the population. This necessarily introduces some sampling error into the estimates, but such sampling error can be quantified.

For large populations, it is usually practically infeasible to examine every single individual in the population of interest, which is the practice of taking a census. In these cases, using sampling and statistical inference is the only alternative to a census.

![
A worker drilling a hole to use for sampling the ice in Lake Michigan. Attempting to take a census of all the ice in Lake Michigan would be practically impossible.<br/>
["Green Bay Ice sampling"](https://www.flickr.com/photos/43788330@N05/8741610624)
by
[NOAA Great Lakes Environmental Research Laboratory](https://www.flickr.com/photos/noaa_glerl/)
is licensed under
[CC BY-SA 2.0](https://creativecommons.org/licenses/by-sa/2.0/)
and is not modified.
](/images/ice-sampling.jpg)

Even in cases in which taking a census is practically possible, it might not be the best way to proceed. While taking a census does eliminate sampling error, it does not eliminate other forms of error. Thus, the resources that might be spent on taking a census might be better spent on other things, because sampling error can be quantified and kept within necessary tolerances.


#### Generalizable Results

In order to generalize a statistic to a larger population, a _representative_ sample must be taken. A sample is representative if every individual in the population had a chance to be included in the sample and the probability of such inclusion is known. In short, a sample is representative if selection for the sample is randomized.

All the individuals that _could_ have been included in the sample constitute the actual _sampled population_. If the statistical analysis done with a representative sample is valid, then the results generalize to this sampled population. _The results do not generalize to other populations._

This is an all too common mistake in interpreting statistics. A study based on a representative sample of women members of an electrical engineering society in the Pacific Northwest of the United States and a study based on a representative sample of women working for financial analysis companies in Queensland, Australia are not results about "women." The results of each study are about two _different_ populations, and there should be no surprise if the two studies arrive at different results.

When you are scrutinizing statistics and encounter a study based on a representative sample, your main task with regard to sampling is to identify what population the results generalize to. The population to which the results generalize is exactly the sampled population &ndash; no more, no less.

For a statistic derived from a telephone survey of area codes in Saint Petersburg, Florida, the results generalize to all those who have access to telephones with phone numbers with a Saint Petersburg area code. The results do not generalize to people living in Papua New Guinea.[^papua] The results do not generalize to people who live in Saint Petersburg, but do not have access to a telephone. The results do generalize to those who live in Saint Petersburg, but only have access to telephones with phone numbers that do not have Saint Petersburg area codes, etc.

[^papua]: The results could hypothetically generalize to individuals living in Papua New Guinea with mobile phones that have Saint Petersburg area code phone numbers. If this were the case, the results would generalize only to these individuals in Papua New Guinea and only if it was still possible to call them at the time the survey was being done.

A subtlety arises when researchers intend, either explicitly or implicitly, to sample one population, but wind up sampling a different population. In these cases, the _target population_ for the study and the sampled population are not exactly the same, though they likely overlap.

This is a phenomenon that often comes up when election predictions fail spectacularly. The target population for a survey that is intended to predict the outcome of an election is all the people who will vote in the election. However, this is a difficult population to identify and to sample.

For instance, nearly all of the surveys leading up to the the 2015 British General Election concluded that the election was going to be a dead heat between the two most popular political parties in the United Kingdom, the Conservative and the Labour parties. However, the election turned out be a clear victory for the Conservative Party. The predictions were so far off that an academic investigation into the widespread mistakes was commissioned. The investigation found that the samples taken were unrepresentative of the actual electorate,[^quota] over-representing Labour voters and under-representing Conservative ones. [@noauthor_general_2016]

[^quota]: Most of these unrepresentative surveys did not using probability sampling, which is the form of sampling described in this article in which every individual in the population has a defined probability of selection for the sample. Instead the surveys, as many market research and political research surveys do, used quota sampling. Quota sampling is a form of sampling that tries to intentionally match certain demographic variables in the sample to the known distributions of the demographic variables in the population. Quota sampling is a lot cheaper because samples can be constructed opportunistically based on individuals who are readily available for survey. However, as the 2015 British General Election illustrates, quota sampling is not very reliable.

![
A map depicting the results of the 2015 General Election in the United Kingdom. Blue represents a victory fot the Conservative Party, and red, a victory for the Labour Party.<br/>
["2015UKElectionMap.svg"](https://commons.wikimedia.org/wiki/File:2015UKElectionMap.svg)
by
Brythones, recolored by Crytographic,
is licensed under
[CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/deed.en)
and is not modified.
](/images/uk-election-map-2015.png)

When scrutinizing statistics, you are concerned with identifying the actual population that was sampled. The statistic can only be generalized to this sampled population. You should not be distracted by the target population that should have been sampled or even the population the researchers try to portray their results as generalizing to. The way to determine this sampled population is by paying close attention to how the sampling was done. The sampled population consists of those individuals that were eligible to be selected for the sample.


#### Unrepresentative Samples

Sometimes, upon discovering the origin of a statistic, you will find that the sample used to calculate the statistic is not representative of any larger population. In this case, the population to which the statistic can be generalized is just the sample itself. In other words, the results do not generalize. This occurs when the sampling is not randomized.

One kind of unrepresentative samples are self-selected samples, such as those composed of volunteers. In these cases, the individuals in the sample were not selected by the researchers with some known probability from a larger population. Instead, the individuals in the sample selected themselves to be included in the sample.

For instance, many psychology experiments are conducted with some number of volunteers taken from the undergraduate student body of the researcher's university. In these cases, results do not generalize even to the student body of the university, let alone to populations outside of the university.

To see why, suppose that a study wanted to determine the average income of students at some university, but used a self-selected sample like those used in many psychology experiments. The students who volunteer for this self-selected sample differ from the general student body in a couple ways. The volunteers have enough time to devote some of their time to participating in a psychology study, but students who work a part-time job and thus have more income also have less time, so might be less likely to volunteer. Furthermore, some of the volunteers might be attracted by the small payment given to those who participate in a psychology study, but students who work a part-time job might be less enticed by this payment since they already have income.

Therefore, there are reasons to believe that the self-selected sample differs from the target population in ways related to the variable of interest. Those who have time to devote to participation in a psychology study for a small payment might have less average income than the student body as a whole. If this were the case, the average income in the self-selected sample would be less than that in population. In this way the self-selected sample would not be representative of the student body. A potential _confounding variable_, i.e., whether or not a student works a part-time job, has been identified.

This is avoided in representative samples by using randomized sampling. For instance, suppose 35\% of students at the university work part-time. In a simple random sample in which all of the students at the university have an equal probability of being selected for the sample,[^simple] the proportion of students in the sample who work a part time job will also be near 35\%. This will be the case not just for this one potentially confounding variable, but for _all_ confounding variables. In this way randomization ensures that a sample will be representative of the population.

[^simple]: Not all randomized sampling consists of simple random samples. Surveys usually use more complex sampling schemes, for various reasons. The sampling is still randomized, but the probabilities of inclusion in the sample are not necessarily all equal. This can be accounted for in the analysis and is a standard practice.


### Randomized, Controlled Experiments

Sometimes statistics are not intended to summarize the state of affairs in a larger population, but are intended to summarize the effect of a specific intervention. For instance, in a clinical trial, volunteers are given an experimental therapy such as a new drug in order to determine whether the therapy works and whether it is safe. The intent of a clinical trial is not to describe properties of a population, such as the prevalence of a disease among citizens of a country. Instead, a clinical trial is concerned with describing the effects of the therapy.

Studies such as described in the previous section on sampling in which no intervention is made are typically called "observational studies," whereas studies in which an intervention is made by the researchers are typically called "experiments."

If a statistic is intended to describe the effect caused by a specific intervention, you should ask questions such as _"Was the intervention compared with a control?"_ and _"Were individuals randomized between the intervention and the control?"_


#### Controls

A _control_ in the context of an experiment is important to determine the effect of the intervention. For example, suppose a group of volunteers suffering from a disease are given a potential new therapy, and afterward variables pertaining to the effects of the disease are measured. Even if some volunteers improved after the therapy, it would not be clear what the effects of the therapy were. Indeed, for many diseases, some proportion of the population suffering from the illness will get better of their own accord over time.

Experiments, therefore, are expected to include a _control group_ that includes individuals given something else other than the intervention of interest. For a disease without a treatment currently, the control group in a clinical trial might be given a placebo, such as an empty capsule, that is known to have no effect on the disease. In other trials for diseases that have existing therapies, the control group will often be given the current best therapy. The intent in these cases is to determine whether or not the new therapy performs better than current best practices.

If you encounter a statistic from an experiment without a control that is claimed to shed light on the effect of some intervention, you should reject it immediately as misleading. Even if 68\% of individuals given snake oil recover from some disease in 10 days, it does not indicate that the snake oil is an effective intervention. For all that is known, 68\% or more of the general population recovers from the disease in 10 days, anyway. Experiments without controls might be useful as exclusively exploratory exercises, but no conclusions about cause and effect can be drawn from them.

![
The cover for Clark Stanley's Snake Oil Liniment, circa 1905. Snake oils were once sold a panaceas despite not having any therapeutic effects, leading to "snake oil" becoming a figure of speech for fradulent products.<br/>
Image was acquired via
[Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Clark_Stanley's_Snake_Oil_Liniment.png) and is in the public domain.
](/images/snake-oil.png)


#### Randomized Assignment

Randomization is important for experiments as it is for observational studies. Again, this is due to the potential effects of confounding variables. In an observational study, a confounding variable can alter the properties of a sample, making it unrepresentative, if the distribution of the confounding variables in the sample differs from its distribution in the population. In experiments, a confounding variable can alter the measured effects of an intervention if the distribution of the confounding variable in the control group differs from the distribution of the confounding variable in the intervention group.

For example, suppose you learn of a new flu therapy that underwent a clinical trial, and the result of the clinical trial found that all indicators of health &ndash; e.g., hospitalization rate, mortality rate, recovery time &ndash; were measurably better in the group given the new flu therapy than in the control group. That might seem promising. However, what if you further learn that the average age in the group given the therapy was 32 and the average age in the control group was 67? In this case, if those who are older are more vulnerable to the effects of the flu, age is a confounding variable. This could easily happen if the experimental assignment was not randomized.

Randomization not only reduces the chances of one confounding variable like age being imbalanced between the intervention and the control groups, but randomization, if done well, balances _all_ confounding variables between the intervention and control groups. Thus, randomized assignment ensures that the effects measured in an experiment are indeed attributable to the intervention as their cause. 


#### Association vs. Causation

Misinterpreting association between two variables as a causal relationship is a classic fallacy in interpreting statistical results that readily leads to numerous examples. The mantra "correlation is not causation" has been drummed into students of statistics courses for decades.

Those who go a hospital are more likely to suffer illness and death than those who do not. Therefore, there is an association between going to a hospital and illness and death. However, one should not conclude that going to a hospital _causes_ illness and death. The obvious confounding variable here is the existence of a prior illness. Generally, healthy people do not go to a hospital, whereas people who are ill often do. Confusing association with causation in this way can lead people to avoid medical care and suffer adverse health and premature death unnecessarily.

Furthermore, a lot of variables undergo trends over time and so can be falsely associated if the trends occur over the same time period. This is a phenomenon that Tyler Vigen has used to humorous effect in his illustrations of associations between obviously unrelated variables, such as divorce rate in Maine and per capita consumption of margarine.

![
[Plot](https://tylervigen.com/spurious-correlations)
by
[Tyler Vigen](https://tylervigen.com/)
is licensed under
[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/).
](/images/margarine-and-divorce.svg)

Invalidly drawing causal conclusions from associations is such a common fallacy that old-fashioned statistics classes sometimes assert that no causal conclusion can be drawn from observational studies and that conclusions about cause and effect can only be made from randomized, controlled trials. For a variety of reasons, this is now largely seen as overly restrictive, and the field of causal inference from observational studies is an emerging field in statistical methodology research.

There is no single recipe for causal inference from observational studies, but what all the methods currently being developed have in common is that they require a lot of work: formalization of concepts and symbology, explicit stating of assumptions, verification that the assumptions are plausible, etc. Because of the newness of these techniques and the amount of effort they require, and because of the abundance of fallacious reasoning that infers causation from mere association, you might be more likely to encounter the fallacy rather than valid causal inference from observational studies. Therefore, it is prudent to treat any conclusion about cause and effect that is derived from a source other than a randomized, controlled experiment with suspicion.


### Summary

 Randomization in sampling and randomization in experimental assignment might seem very similar. They are both practices that use randomization to address issues that stem from confounding variables. However, they are necessary for two different kinds of inference, and the consequences of these two kinds of randomization are independent of each other.[^randomization] Thus, a study might have randomized sampling, randomized assignment, both, or neither. The ramifications for a study are sometimes summarized in a table such as Table 1.

[^randomization]: A point of confusion can arise because randomized experiments are often analyzed using the same mathematical tools as observational studies. Classical statistical inference was developed using the mathematical model of a population and a sample from the population. Rather than reinvent all of the statistical tools developed over the years, analysis of randomized, controlled experiments sometimes simply _invokes_ a population model and reuses the same tools. While this practice is common, inference for randomized, controlled experiments does not necessarily have to be done this way. Rosenberger and Lachin [-@rosenberger_randomization_2015] summarize inference based on the randomization used for assignment in an experiment, instead of invoking a population model.

<p class="caption">Table 1: A summary of scope of inference for statistical conclusions based on what randomization was used.</p>

<table class="contingency">
  <thead>
    <tr class="question-row">
      <th class="blank"></th>
      <th class="blank"></th>
      <th class="column-question" colspan=2>Was this a randomized, controlled experiment?</th>
    </tr>
    <tr>
      <th class="blank"></th>
      <th class="blank"></th>
      <th class="column-label">No</th>
      <th class="column-label">Yes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th class="row-question" rowspan=2>Was the sampling randomized?</th>
      <td class="row-label">Yes</td>
      <td class="count">Conclusions generalize to population, but no conclusions about cause should be made.</td>
      <td class="count">Conclusions generalize to population, and conclusions about cause can be made.</td>
    </tr>
    <tr>
      <td class="row-label">No</td>
      <td class="count">Conclusions do not generalize to population, and no conclusions about cause should be made.</td>
      <td class="count">Conclusions do not generalize to population, but conclusions about cause can be made.</td>
    </tr>
  </tbody>
</table>

The important takeaway for the work of scrutinizing statistics that you encounter is that there are two separate questions to address about any given statistic. One question is whether or not a statistic generalizes to a larger population, which can be answered in the affirmative if it is based on a representative &ndash; and hence randomized &ndash; sample. Another question is whether or not a statistic is indicative of the effect of an identifiable cause, which can be answered in the affirmative if the statistic is derived from a randomized, controlled experiment.


[^generic]: Generic comparisons assert that there is a difference in some variable between two populations, but do not quantify how much of a difference there is, such as when it is asserted that some group has "more" or "less" of something than another group. This is flawed and deceptive for numerous reasons and is the topic of its own article. 


## Citations

::: {#refs}
:::


## Footnotes
