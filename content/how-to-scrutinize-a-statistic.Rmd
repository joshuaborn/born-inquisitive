---
draft: true
title: "How to Scrutinize a Statistic"
date: 2021-12-01
tags: ["reasoning about evidence"]
bibliography: "../bibliographies/how-to-scrutinize-a-statistic.bib"
csl: "../bibliographies/chicago-author-date.csl"
link-citations: true
implicit-figures: true
output:
  blogdown::html_page:
    md_extensions: ["+footnotes"]
    toc: true
    toc_depth: 5
include-before: |
  <p>Opening paragraph goes here.</p>
  <!--more-->
  ## Table of Contents
---

```{r setup, echo=FALSE, message=FALSE}
```


## Introduction

There is a cautionary saying that warns to be wary of those who use statistics like a drunkard uses a lamppost &ndash; for support rather than for illumination.[^saying]

[^saying]: This saying was popularized in 1937, when it was commonly attributed to Scottish folklorist Andrew Lang. Andrew Lang died in 1912 and may or may not have originated it, as it does not appear in any of his extant writings. Regardless of the origin of this specific version of the saying, it was likely inspired by similar figures of speech which came before it. See [this article](https://quoteinvestigator.com/2014/01/15/stats-drunk/) at the Quote Investigator blog for more information about the origin of the saying.

![
A man literally using a lamppost for support, which is a metaphor used in a popular saying describing the misuse of statistics.<br/>
["Lamppost Man"](https://www.flickr.com/photos/78019868@N05/741397893)
by
[David Hodgson](https://www.flickr.com/photos/publicplaces/)
is licensed under
[CC BY 2.0](https://creativecommons.org/licenses/by/2.0/)
and is not modified.
](/images/lamppost-man.jpg)

The point of this saying is to warn about those who start with a preconceived idea of a conclusion that they want to reach and then search for statistics that appear to validate that preconceived conclusion.

This is a practice well known to statisticians, so much so that _How to Lie with Statistics_ [-@huff_how_1993] is still in print more than fifty years after it was originally published in 1954.

The practice is effective for at least three reasons.

First, statistics have some persuasive ability. They bring with them an atmosphere of scholarship, science, and quantitative certainty. Even when no relevant empirical observations have been made, citing statistics make it appear that a position is supported by empirical evidence.

Second, given enough time and effort, statistics can be manufactured that appear to support any arbitrary position. This can be out of intentional deception by bad actors. It can also be done accidentally by those who do not know how to do empirical inquiry well, by those afflicted by the psychology of motivated reasoning, or by researchers who out of desperation to further their academic careers in a "publish or perish" world, publish whatever they can.

![
A stressed young woman hunched over a laptop computer. Aspiring academicians are judged by the number of publications they have and the prestige of the journals in which the publications appear, while often competing for very few openings, a phenomenon called "publish or perish" colloquially.<br/>
[Image](https://pixabay.com/photos/laptop-woman-education-study-young-3087585/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3087585)
by
[Jan Va≈°ek](https://pixabay.com/users/jeshoots-com-264599/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3087585)
from
[Pixabay](https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3087585).
](/images/stressed-student.jpg)

Third, interpreting and scrutinizing statistics requires knowledge and skills that many laypeople do not possess. Indeed, those who have had education in statistics are sometimes prone to mistakes in using and interpreting statistical methods. Thus, misuses of statistics can go unnoticed, by the general public or by academic peer review. Indeed, as shall be seen, some misuses of statistics can actually become entrenched in academic practice.

This situation might seem so discouraging that you might be inclined to throw out the use of statistics altogether. However, this would be a rash mistake. Those who care about any social issues, medical issues, environmental issues, etc, are necessarily concerned about understanding phenomena that occur in large populations of individuals. Therefore, if you care about such issues, you are necessarily engaged in a statistical inquiry, whether you realize this or not.

The solution for those who care about such issues, therefore, is to be competent at scrutinizing statistics. The alternative is to be preyed upon by charlatans or, perhaps worse, to become a charlatan yourself.

Another way to conceive of this is that _statistics should be questioned,_ always, by every person who encounters them and who must make a decision on whether or not to believe what is being reported.


### Purpose of This Article

The intent of this article is to assist the reader in the enterprise of questioning the statistics you encounter. It does not assume the reader has had any formal instruction in statistics, though such instruction would be valuable in understanding the concepts used throughout. The only prerequisite for this article is cursory mathematics knowledge at the secondary school level.

This article is not a replacement for courses in statistics. However, the converse is also true. Statistics courses are rarely taught from the standpoint of what questions are relevant to ask when encountering a statistic. Rather, introductory statistics courses usually focus on giving students a bag of statistical tools to use in their research activities.[^courses]

[^courses]: Statistics courses beyond the introductory level usually have another kind of content. Typically, introductory courses are labeled "applied statistics" and are concerned with how to use various statistical methods. Upper-level courses are usually termed "mathematical statistics" and are concerned with the mathematical foundations of results used in statistics and how the statistical techniques are thus derived.

There is merit to knowledge of this bag of tools even if you do not do empirical research in your own life. For instance, when the toxicity of something is reported, it will often be reported by the the median lethal dose (LD<sub>50</sub>) statistic. The LD<sub>50</sub> in turn is usually derived by way of logistic regression, which is one of the tools in the bag of tools typically covered in a two semester introductory statistics sequence at the university level. Therefore, if you want to understand where this statistic comes from, you need to understand logistic regression.

This article, however, instead of discussing any one specific statistical technique, discusses questions that should be asked of all statistics regardless of how they are derived. Thus, the content of this article is both different from and complementary to the content of statistics courses.


## Provenance

The first question to ask of any statistic is in regard to its _provenance._ The word "provenance" is borrowed from historians of art who use it to refer to the chronology of place and custody of a work of art traced back to its origin, though the word is now used by a variety of fields.

![
Three paintings on display in the National Gallery of Art in the United States. The practice of investigating the provenance of works of art has led to the word "provenance" being used in other contexts.<br/>
["Art Gallery"](https://www.flickr.com/photos/55267995@N04/14288414720)
by
[cjt71081](https://www.flickr.com/photos/55267995@N04/)
is licensed under
[CC BY-SA 2.0](https://creativecommons.org/licenses/by-sa/2.0/)
and is not modified.
](/images/national-gallery-of-art.jpg)

In the context of scrutinizing a statistic, determining its provenance consists of asking questions such as _"Where does this statistic come from?" "Who made the observations from which this statistic was derived?"_ and _"How were those observations made?"_

The reason this is the first question to ask of any statistic is that its answer is a prerequisite for subsequent questions to ask when scrutinizing a statistic. If you do not know where a statistic came from or how it was derived, you cannot scrutinize further.

One good thing about establishing the provenance of a statistic versus other kinds of provenance is that many if not most statistics of any quality are reported in a scholarly paper of some kind. Thus, the work of establishing provenance of a statistic usually ends with a very definitive result: a document that has a Digital Object Identifier [@international_doi_foundation_digital_2021] and that can be succinctly referenced in any one of a number of standardized citation formats.

This first question is phrased as "establishing provenance" and not simply "looking it up" because there are many situations where the mere act of determining where a statistic comes from is itself a fair amount of work.


### Lack of a Source

The most obvious situation that makes establishing provenance challenging is when a statistic is stated, but no source is referenced.

![
A sign that reads "citation needed" at a rally in Washington, DC, alluding to the tag used in Wikipedia for assertions made without any source given.<br/>
["Citation needed"](https://www.flickr.com/photos/87913776@N00/5129607997)
by
[futureatlas.com](https://www.flickr.com/photos/87913776@N00/)
is licensed under
[CC BY 2.0](https://creativecommons.org/licenses/by/2.0/)
and is not modified.
](/images/citation-needed.jpg)

This can occur in a variety of settings, such as when reading an article or when engaging in face-to-face conversation. If the medium allows for two-way communication, it might be prudent to ask the person stating the statistic where that person encountered the statistic.

If no source is provided, that immediately casts doubt on the standards of the person stating the statistic. Such a person has not done the due diligence of scrutinizing a statistic, but is still passing it along, which is a practice that can spread misinformation.

However, while the lack of a source casts doubt on the veracity of the person asserting a statistic, it does not provide insight on the veracity of the statistic itself. In these cases, the best you can do is withhold any judgment, either assenting or denying, and try to find the origin of the statistic yourself.

Today, with the plethora of Internet search tools available, the task of finding an origin is not impossible. It is simply a value judgment you must make as to whether you want to spend your time in this manner. It is certainly within the realm of reason to remain in a state of suspended judgment if your time is better spent on other matters.


### Misrepresented Source

When a source is provided, the obvious first step in establishing provenance is to check this source. However, sometimes this immediately becomes a dead end because the cited source does not actually assert the statistic in question.  This phenomenon can have a variety of causes.

For one, the person falsely citing a source to be the origin of a statistic might simply be making a mistake in good faith. Perhaps the person [remembered something incorrectly](/anecdotes-are-not-evidence.html#flaws-of-memory) or confused this source with another.

Alternatively, some people engage in this practice intentionally as an act of deception. The mere presence of a falsely cited source can lend a statistic an air of credibility it would not have otherwise because many people do not check sources.

Finally, there are whole branches of thought that do not believe they need empirical evidence. Thus, a cited source might be asserting something similar to a statistic, but if the source in question is based on ideology instead of empirical evidence, then this pseudo-statistic is just made-up. However, a gullible audience might treat it as a source.

In the first case of a simple mistake, you are left in the same state as you would have been if no source was provided.

In the second case of intentional deception or the third case of an ideological origin, you have strong evidence that the statistic in question was just made-up and can be dismissed. There is a subtlety here to be concerned with, nonetheless. Even if a particular made-up statistic is false, that tells you nothing about what a true statistic would be. For instance, if someone makes up a statistic that 80\% of the _cagot_ ethnic group live in poverty, and you find out this is just made-up, you still do not know what the actual poverty rate among the the _cagot_ is.[^cagot]

[^cagot]: The _cagot_ were a real ethnic group who lived in a region part of modern day Spain and France. People categorized as _"cagot"_ were the object of much prejudice, and their origins have remained a mystery to scholarship over the centuries.

Unfortunately, when either unsourced statistics or statistics with a misrepresented source are encountered, they leave you not having learned any new information. The exercise of encountering them is not entirely a waste of your time, however, if you take the view that they are opportunities to develop your skills at scrutinizing statistics.


### Chain of References

Sometimes you find that the source cited for a statistic, when checked, itself cites another source for the statistic in question. That source might then cite some other source, and that source might yet again cite another source.

Especially whenever Internet journalism churns out numerous shallow articles, this chain of references can get quite long, and this phenomenon can make the work of establishing provenance needlessly time consuming and tedious. However, all that is important in this process is identifying the origin of the statistic. The chain of references themselves neither add nor detract from the credibility of the statistic. Indeed, these chains of references could be avoided if anyone contributing to the chain did their due diligence by establishing provenance and including a reference to the original source.

This highlights yet another fallacy that is sometimes used by those who use statistics for support rather than illumination. You should not be impressed when a single statistic has numerous citations included with it. If anything, seeing multiple sources cited for the same statistic should raise your suspicion.

For one, citing multiple supposed sources is easily accomplished by citing every step in a chain of references to a single origin as if they were a separate source. If they all ultimately lead to the same origin, then there is no point in this multiplicity of citations, and using multiple citations like this is deceptive.

Secondly, for multiple citations that lead to different origins, representing them as if they have the same conclusion is an indication that the sources are being misrepresented. It is unlikely that several studies would arrive at the exact same statistic, which is discussed further in the sections on scope of inference, effect size, and estimation error. Thus, when multiple original sources are cited for the same statistic, it is likely results are being treated as if they were equivalent when they are not.


### Primary versus Secondary Sources

Encyclopedias such as Wikipedia can be helpful when establishing provenance, but with one major caveat. Establishing the provenance of a statistic as discussed here is tracing the statistic back to its _origin,_ which is a _primary source._  Wikipedia has a different mission than establishing provenance, and prefers _secondary sources,_ as can be read its documentation:

> Wikipedia articles should be based on reliable, published secondary sources and, to a lesser extent, on tertiary sources and primary sources. . . . A secondary source provides an author's own thinking based on primary sources, generally at least one step removed from an event. It contains an author's _analysis, evaluation, interpretation, or synthesis_ of the facts, evidence, concepts, and ideas taken from primary sources. [@noauthor_wikipediano_2021]

In this article's context, a secondary source does the work of scrutinizing a statistic, which is what you the reader are supposed to be doing. A secondary source might be useful for comparison, but is not a replacement for finding the primary source.

That being understood, Wikipedia does contain a lot of references to primary sources, as well, even though its "no original research" policy discourages their use except under limited circumstances. Furthermore, a good secondary source will reference the primary source, and so might be a useful step along a chain of references in order to establish provenance.


## Scope of Inference

Once you have found the origin of a statistic, the next two genres of questions to ask about the statistic are both related to what is termed "scope of inference" in statistics courses. Scope of inference is divided into two concerns: whether or not the results of a study generalize to a larger population, and whether or not conclusions can be drawn about cause and effect. The former is a concern about sampling, and the latter is a concern about experimental assignment. Both of these concerns stem from the possibility of confounding variables, and both concerns are addressed by a form of randomization.


### Sampling

Sometimes a statistic is intended to summarize information about the state of affairs in a large population. For instance, a country might be interested in the poverty rate of its citizens, which can constitute a population of millions of individual people. If the statistic is intended to generalize, you should ask questions such as _"What population was sampled?"_ and _"How was the sampling done?"_

The main approach used to infer information about a large population is to take a representative sample from the population and use measurements of the sample to infer estimates about the population. This necessarily introduces some sampling error into the estimates, but such sampling error can be quantified.

For large populations, it is usually practically infeasible to examine every single individual in the population of interest, which is the practice of taking a census. In these cases, using sampling and statistical inference is the only alternative to a census.

![
A worker drilling a hole to use for sampling the ice in Lake Michigan. Attempting to take a census of all the ice in Lake Michigan would be practically impossible.<br/>
["Green Bay Ice sampling"](https://www.flickr.com/photos/43788330@N05/8741610624)
by
[NOAA Great Lakes Environmental Research Laboratory](https://www.flickr.com/photos/noaa_glerl/)
is licensed under
[CC BY-SA 2.0](https://creativecommons.org/licenses/by-sa/2.0/)
and is not modified.
](/images/ice-sampling.jpg)

Even in cases in which taking a census is practically possible, it might not be the best way to proceed. While taking a census does eliminate sampling error, it does not eliminate other forms of error. Thus, the resources that might be spent on taking a census might be better spent on other things, because sampling error can be quantified and kept within necessary tolerances.


#### Generalizable Results

In order to generalize a statistic to a larger population, a _representative_ sample must be taken. A sample is representative if every individual in the population had a chance to be included in the sample and the probability of such inclusion is known. In short, a sample is representative if selection for the sample is randomized.

All the individuals that _could_ have been included in the sample constitute the actual _sampled population_. If the statistical analysis done with a representative sample is valid, then the results generalize to this sampled population. _The results do not generalize to other populations._

This is an all too common mistake in interpreting statistics. A study based on a representative sample of women members of an electrical engineering society in the Pacific Northwest of the United States and a study based on a representative sample of women working for financial analysis companies in Queensland, Australia are not results about "women." The results of each study are about two _different_ populations, and there should be no surprise if the two studies arrive at different results.

When you are scrutinizing statistics and encounter a study based on a representative sample, your main task with regard to sampling is to identify what population the results generalize to. The population to which the results generalize is exactly the sampled population &ndash; no more, no less.

For a statistic derived from a telephone survey of area codes in Saint Petersburg, Florida, the results generalize to all those who have access to telephones with phone numbers with a Saint Petersburg area code. The results do not generalize to people living in Papua New Guinea.[^papua] The results do not generalize to people who live in Saint Petersburg, but do not have access to a telephone. The results do generalize to those who live in Saint Petersburg, but only have access to telephones with phone numbers that do not have Saint Petersburg area codes, etc.

[^papua]: The results could hypothetically generalize to individuals living in Papua New Guinea with mobile phones that have Saint Petersburg area code phone numbers. If this were the case, the results would generalize only to these individuals in Papua New Guinea and only if it was still possible to call them at the time the survey was being done.

A subtlety arises when researchers intend, either explicitly or implicitly, to sample one population, but wind up sampling a different population. In these cases, the _target population_ for the study and the sampled population are not exactly the same, though they might overlap.

This is a phenomenon that often comes up when election predictions fail spectacularly. The target population for a survey that is intended to predict the outcome of an election is all the people who will vote in the election. However, this is a difficult population to identify and to sample.

For instance, nearly all of the surveys leading up to the the 2015 British General Election concluded that the election was going to be a dead heat between the two most popular political parties in the United Kingdom, the Conservative and the Labour parties. However, the election turned out be a clear victory for the Conservative Party. The predictions were so far off that an academic investigation into the widespread mistakes was commissioned. The investigation found that the samples taken were unrepresentative of the actual electorate,[^quota] over-representing Labour voters and under-representing Conservative ones. [@noauthor_general_2016]

[^quota]: Most of these unrepresentative surveys did not using probability sampling, which is the form of sampling described in this article in which every possible sample taken from the population has a defined probability of selection. Instead the surveys, as many market research and political research surveys do, used quota sampling. Quota sampling is a form of sampling that tries to intentionally match certain demographic variables in the sample to the known distributions of the demographic variables in the population. Quota sampling is a lot cheaper because samples can be constructed opportunistically based on individuals who are readily available. However, as the 2015 British General Election illustrates, quota sampling is not very reliable.

![
A map depicting the results of the 2015 General Election in the United Kingdom. Blue represents a victory fot the Conservative Party, and red, a victory for the Labour Party.<br/>
["2015UKElectionMap.svg"](https://commons.wikimedia.org/wiki/File:2015UKElectionMap.svg)
by
Brythones, recolored by Crytographic,
is licensed under
[CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/deed.en)
and is not modified.
](/images/uk-election-map-2015.png)

When scrutinizing statistics, you are concerned with identifying the actual population that was sampled. The statistic can only be generalized to this sampled population. You should not be distracted by the target population that should have been sampled or even the population the researchers try to portray their results as generalizing to. The way to determine this sampled population is by paying close attention to how the sampling was done. The sampled population consists of those individuals that were eligible to be selected for the sample.


#### Unrepresentative Samples

Sometimes, upon discovering the origin of a statistic, you will find that the sample used to calculate the statistic is not representative of any larger population. In this case, the population to which the statistic can be generalized is just the sample itself. In other words, the results do not generalize. This occurs when the sampling is not randomized.

One kind of unrepresentative samples are self-selected samples, such as those composed of volunteers. In these cases, the individuals in the sample were not selected by the researchers with some known probability from a larger population. Instead, the individuals in the sample selected themselves to be included in the sample.

For instance, many behavioral and social science experiments are conducted with some number of volunteers taken from the undergraduate student body of the researcher's university. In these cases, results do not generalize even to the student body of the university, let alone to populations outside of the university.

To see why, suppose that a study wanted to determine the average income of students at some university, but used a self-selected sample like those used in many social and behavioral science experiments. The students who volunteer for this self-selected sample differ from the general student body in a couple ways. The volunteers have enough time to devote some of their time to participating in a study, but students who work a part-time job and thus have more income also have less time, so might be less likely to volunteer. Furthermore, some of the volunteers might be attracted by the small payment given to those who participate in such a study, but students who work a part-time job might be less enticed by this payment since they already have income.

Therefore, there are reasons to believe that the self-selected sample differs from the target population in ways related to the variable of interest. Those who have time to devote to participation in a behavioral or social science study for a small payment might have less average income than the student body as a whole. If this were the case, the average income in the self-selected sample would be less than that in population. In this way the self-selected sample would not be representative of the student body. A potential _confounding variable_, i.e., whether or not a student works a part-time job, has been identified.[^self-selected]

[^self-selected]: For more discussion of why self-selected samples are unrepresentative, see Chapter 1 of Lohr [-@lohr_sampling_2021].

This is avoided in representative samples by using randomized sampling. For instance, suppose 35\% of students at the university work part-time. In a simple random sample in which all of the students at the university have an equal probability of being selected for the sample,[^simple] the proportion of students in the sample who work a part time job will also be near 35\%. This will be the case not just for this one potentially confounding variable, but for _all_ confounding variables. In this way randomization ensures that a sample will be representative of the population.

[^simple]: Not all randomized sampling consists of simple random samples. Surveys usually use more complex sampling schemes, for various reasons. The sampling is still randomized, but the probabilities of inclusion in the sample are not necessarily all equal. This can be accounted for in the analysis and is a standard practice. [@lohr_sampling_2021]


### Randomized, Controlled Experiments

Sometimes statistics are not intended to summarize the state of affairs in a larger population, but are intended to summarize the effect of a specific intervention. For instance, in a clinical trial, volunteers are given an experimental therapy such as a new drug in order to determine whether the therapy works and whether it is safe. The intent of a clinical trial is not to describe properties of a population, such as the prevalence of a disease among citizens of a country. Instead, a clinical trial is concerned with describing the effects of the therapy.

Studies such as described in the previous section on sampling in which no intervention is made are typically called "observational studies," whereas studies in which an intervention is made by the researchers are typically called "experiments."

If a statistic is intended to describe the effect caused by a specific intervention, you should ask questions such as _"Was the intervention compared with a control?"_ and _"Were individuals randomized between the intervention and the control?"_


#### Controls

A _control_ in the context of an experiment is important to determine the effect of the intervention. For example, suppose a group of volunteers suffering from a disease are given a potential new therapy, and afterward variables pertaining to the effects of the disease are measured. Even if some volunteers improved after the therapy, it would not be clear what the effects of the therapy were. Indeed, for many diseases, some proportion of the population suffering from the illness will get better of their own accord over time.

Experiments, therefore, are expected to include a _control group_ that includes individuals given something else other than the intervention of interest. For a disease without a treatment currently, the control group in a clinical trial might be given a placebo, such as an empty capsule, that is known to have no effect on the disease. In other trials for diseases that have existing therapies, the control group will often be given the current best therapy. The intent in these cases is to determine whether or not the new therapy performs better than current best practices.

If you encounter a statistic from an experiment without a control that is claimed to shed light on the effect caused by an intervention, you should reject it immediately as misleading. Even if 68\% of individuals given snake oil recover from some disease in 10 days, it does not indicate that the snake oil is an effective intervention. For all that is known, 68\% or more of the general population recovers from the disease in 10 days, anyway. Experiments without controls might be useful as exclusively exploratory exercises, but no conclusions about cause and effect can be drawn from them.

![
The cover for Clark Stanley's Snake Oil Liniment, circa 1905. Snake oils were once sold a panaceas despite not having any therapeutic effects, leading to "snake oil" becoming a figure of speech for fradulent products.<br/>
Image was acquired via
[Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Clark_Stanley's_Snake_Oil_Liniment.png) and is in the public domain.
](/images/snake-oil.png)


#### Randomized Assignment

Randomization is important for experiments as it is for observational studies. Again, this is due to the potential effects of confounding variables. In an observational study, a confounding variable can alter the properties of a sample, making it unrepresentative, if the distribution of the confounding variables in the sample differs from its distribution in the population. In experiments, a confounding variable can alter the measured effects of an intervention if the distribution of the confounding variable in the control group differs from the distribution of the confounding variable in the intervention group.

For example, suppose you learn of a new flu therapy that underwent a clinical trial, and the result of the clinical trial found that all indicators of health &ndash; e.g., hospitalization rate, mortality rate, recovery time &ndash; were measurably better in the group given the new flu therapy than in the control group. That might seem promising. However, what if you further learn that the average age in the group given the therapy was 32 and the average age in the control group was 67? In this case, if those who are older are more vulnerable to the effects of the flu, age is a confounding variable. This could easily happen if the experimental assignment was not randomized.

Randomization not only reduces the chances of one confounding variable like age being imbalanced between the intervention and the control groups, but randomization, if done well, balances _all_ confounding variables between the intervention and control groups. Thus, randomized assignment ensures that the effects measured in an experiment are indeed caused by the intervention. 


#### Association versus Causation

Misinterpreting association between two variables as a causal relationship is a classic fallacy in interpreting statistical results. The mantra "correlation is not causation" has been drummed into students of statistics courses for decades.

Those who go a hospital are more likely to suffer illness and death than those who do not. Therefore, there is an association between going to a hospital and illness and death. However, one should not conclude that going to a hospital _causes_ illness and death. The obvious confounding variable here is the existence of a prior illness. Generally, healthy people do not go to a hospital, whereas people who are ill often do. Confusing association with causation in this way can lead people to avoid medical care and suffer adverse health and premature death unnecessarily.

Furthermore, a lot of variables undergo trends over time and so can be falsely associated if the trends occur over the same time period. This is a phenomenon that Tyler Vigen has used to humorous effect in his illustrations of associations between obviously unrelated variables, such as divorce rate in Maine and per capita consumption of margarine.

![
[Plot](https://tylervigen.com/spurious-correlations)
by
[Tyler Vigen](https://tylervigen.com/)
is licensed under
[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/).
](/images/margarine-and-divorce.svg)

Invalidly drawing causal conclusions from associations is such a common fallacy that old-fashioned statistics classes sometimes assert that no causal conclusion can be drawn from observational studies and that conclusions about cause and effect can only be made from randomized, controlled trials. For a variety of reasons, this is now largely seen as overly restrictive, and the field of causal inference from observational studies is an emerging field in statistical methodology research.

There is no single recipe for causal inference from observational studies, but what all the methods currently being developed have in common is that they require a lot of work: formalization of concepts, explicit stating of assumptions, verification that the assumptions are plausible, etc. Because of the newness of these techniques and the amount of effort they require, and because of the abundance of fallacious reasoning that infers causation from mere association, it is prudent to treat any conclusion about cause and effect that is derived from a source other than a randomized, controlled experiment with suspicion.


### Summary

Randomization in sampling and randomization in experimental assignment might seem very similar. They are both practices that use randomization to address issues that stem from confounding variables. However, they are necessary for two different kinds of inference, and the consequences of these two kinds of randomization are independent of each other.[^randomization] Thus, a study might have randomized sampling, randomized assignment, both, or neither. The ramifications for a study are sometimes summarized in a table such as Table 1.

[^randomization]: A point of confusion can arise because randomized experiments are often analyzed using the same mathematical tools as observational studies. Classical statistical inference was developed using the mathematical model of a population and a sample from the population. Rather than reinvent all of the statistical tools developed over the years, analysis of randomized, controlled experiments sometimes simply _invokes_ a population model and reuses the same tools. While this practice is common, inference for randomized, controlled experiments does not necessarily have to be done this way. Rosenberger and Lachin [-@rosenberger_randomization_2015] summarize inference based on the randomization used for assignment in an experiment, instead of invoking a population model.

<p class="caption">Table 1: A summary of scope of inference for statistical conclusions based on what randomization was used.</p>

<table class="contingency">
  <thead>
    <tr class="question-row">
      <th class="blank"></th>
      <th class="blank"></th>
      <th class="column-question" colspan=2>Was this a randomized, controlled experiment?</th>
    </tr>
    <tr>
      <th class="blank"></th>
      <th class="blank"></th>
      <th class="column-label">No</th>
      <th class="column-label">Yes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th class="row-question" rowspan=2>Was the sampling randomized?</th>
      <td class="row-label">Yes</td>
      <td class="count">Conclusions generalize to population, but no conclusions about cause should be made.</td>
      <td class="count">Conclusions generalize to population, and conclusions about cause can be made.</td>
    </tr>
    <tr>
      <td class="row-label">No</td>
      <td class="count">Conclusions do not generalize to population, and no conclusions about cause should be made.</td>
      <td class="count">Conclusions do not generalize to population, but conclusions about cause can be made.</td>
    </tr>
  </tbody>
</table>

The important takeaway for the work of scrutinizing statistics that you encounter is that there are two separate questions to address about any given statistic. One question is whether or not a statistic generalizes to a larger population, which can be answered in the affirmative if it is based on a representative &ndash; and hence randomized &ndash; sample. Another question is whether or not a statistic is indicative of the effect of an identifiable cause, which can be answered in the affirmative if the statistic is derived from a randomized, controlled experiment.


## Practical Significance

Once you have identified the origin of a statistic and you have determined the statistic's scope of inference, the next genre of questions you should ask about a statistic are _"What is the practical significance of this statistic?"_ and _"How much of an effect size is described by this statistic?"_


### Subject-Matter Knowledge

Whether a statistic is practically significant is less a matter of statistical theory and much more a function of knowledge of the subject-matter of the context for the statistic.

For instance, suppose you are developing a new drug intended to be used as an alternative to insulin therapy for those with diabetes mellitus. Those with insulin-dependent diabetes mellitus (IDDM) would, without medical intervention, have levels of daily blood glucose higher than what is typical. However, medical research has found that IDDM patients have long-term health benefits if they keep their daily blood glucose levels lowered to within typical ranges. [@diabetes_effect_1993] This new drug you are developing could therefore be of some use.

However, if you discover the drug decreases daily mean blood glucose by only 5 milligrams per decaliter (mg/dl) on average, its effects are not practically significant, because prior research observed long-term health benefits occurred when IDDM patients lowered their daily mean blood glucose from levels around 230 mg/dl to levels around 130 mg/dl.

Thus, the knowledge used to evaluate the practical significance of the statistic in this example comes from subject-matter knowledge pertaining to diabetes, not from any particular statistical tool.


### Comparison

The previous example also illustrates another phenomenon: the practical significance of a statistic comes from its comparison to something else. In the diabetes example, the average change in blood glucose levels caused by the drug is evaluated by comparing it to the therapeutic change in blood glucose levels observed in a previous study. A single statistic in isolation without any contextual knowledge has no practical significance.

For example, suppose you learn that the per capita expenditure on food and beverages for off-premises consumption in Michigan during 2020 was \$3,532. [@zemanek_personal_2021, Table 4] Unless you have detailed economic knowledge of the United States, this statistic probably has very little practical significance to you. Is \$3,532 a lot? Is \$3,532 a little?[^relative] This statistic, without any context, is impossible to interpret.

[^relative]: As it turns out, Michigan was right in the middle of per capita expenditure on food and beverages for off-premises consumption compared with other states. [@zemanek_personal_2021, Table 4]

With other knowledge for comparison, you can discover the practical significance of the statistic. The \$3,532 per capita expenditure on food and beverages for off-premises consumption in Michigan during 2020 was a 10.6% increase from the previous year, despite there being a decrease of 22.6% in the amount spent on food services and accommodations. [@zemanek_personal_2021, Table 2] This is consistent with the hypothesis that people in Michigan were dining in restaurants less and eating at home more during the COVID-19 pandemic.


### Generic Comparisons

Because comparison is the basis for establishing practical significance, you will sometimes encounter the maddening practice of asserting that one population has "more" of some quantity than another or that a population has "less" of a quantity than another.

Such assertions are almost worse than no information at all because they create the impression of practical significance while withholding the very information you need to evaluate practical significance, namely, _how much more_ or _how much less._

This is the practice of _generic comparisons._ Generic comparisons assert that there is a difference in some variable between two populations, but do not quantify how much of a difference there is.

Generic comparisons can be used deceptively. You could, for instance, assert that patients given the prospective insulin-alternative drug discussed in the earlier example have a _lower_ mean daily mean blood glucose than control group patients, and leave it at that. However, you know very well that the actual difference in mean blood glucose caused by the drug is not practically significant for IDDM patients. By withholding quantification from the comparison between patients given the drug and patients in the control group, you can pretend there is practical significance.

Therefore, whenever you encounter an assertion that there is "more" or "less" of something in one population than in another &ndash; or that levels are "higher" or "lower," "bigger" or "smaller," "greater" or "lesser," etc &ndash; the very next question that you should demand is _"How much more?"_ or  _"How much less?"_


### Effect Size

A quick answer to such questions as _"How much more?"_ has come to be commonly called "effect size." This is an unfortunate phrase, because it evokes thinking about cause and effect. This is fine if the scope of inference for a statistic is that of a randomized, controlled trial or if the statistic results from causal inference techniques applied to observational studies. However, remember that in purely observational studies, only an association, not causation, can be inferred.

The various ways in which two populations can have "more" or "less" of some variable is a nontrivial question. However, even without going very deep into comparing the distribution of a variable between two populations, you can achieve a solid first impression by considering an appropriate summary statistic of effect size. In order to understand what sort of summary statistic to look for, you must understand what are the different types of variables you encounter.


#### Types of Variables

In statistical analysis, a variable of interest is often called a "response variable." A response variable is analyzed in comparison to another variable often called an "explanatory variable."[^independent] Some amount of variation in a response variable is suspected to be on account of variation in the explanatory variable.

[^independent]: Response variables are sometimes called "research variables" or "dependent variables," and explanatory variables are sometimes called "independent variables." However, this article recommends avoiding the use of the labels "dependent variable" and "independent variable" because "independence" means something else very important in statistical theory. Indeed, many statistical models and tests are built upon an assumption of independence. While the violation of other assumptions, such as the assumption of normality, can still result in passable estimates, failure to handle the autocorrelation structure in the observations when the assumption of independence is violated will usually make estimates of standard errors drastically mistaken and thus invalidate the results.

For instance, in the insulin-alternative drug example, a clinical trial for the new drug would have the response variable of a patient's mean daily blood glucose level. The explanatory variable in this case would be whether or not a patient is given the new drug.

Both response and explanatory variables can come in different types. Broadly, there are two types of variables that you might encounter: categorical variables and numerical variables.[^variables] Categorical variables are the sort in which observations consist of assignment to one of a number of discrete levels such as _small, medium,_ or _large,_ whereas numerical variables are the sort in which observations consist of assignment to a value on the number line, such as 1.7, 42.4, or 1,852.

[^variables]: Even within these two broad types of variables, there are more subtypes, but for the purposes of this article, only these two types are used. Also, it should noted that in the math underlying statistical theory, categorical variables are often taken as just a more limited case of numerical variables, and the proofs and derivations for numerical variables reused for categorical variables. However, the way the two kinds of variables are thought about and handled in applied statistical analysis can be quite different.

Whether a response variable is categorical or numerical is a separate question from whether an explanatory variable is categorical or numerical. In the insulin-alternative drug example, the response variable is a numerical variable because numbers in the hundreds of mg/dl are used. However, the explanatory variable is a categorical variable with just two levels: _given the drug_ or _not given the drug._

Categorical variables are typically summarized using either counts or proportions. For instance, using raw counts, a warehouse might have 325 small sized garments, 450 medium sized garments, and 375 large sized garments. Using proportions, about 28.3\% of the clothes at the warehouse are size small, 39.1\% are size medium, and 32.6\% are size large.

Similarly, numerical variables are typically summarized using either totals or arithmetic means. For instance, the total value of inventory at the warehouse might be \$34,500. Using an arithmetic mean, the average value of an item at the warehouse is  \$30, if there are 1,150 items in the warehouse.


#### Missing Denominators

When the explanatory variable is categorical, you are essentially comparing the response variable between two or more populations, one population for each level of the explanatory variable.

When comparing between two or more populations, proportions or means are usually preferable to counts or totals.[^totals] This is because for many variables, counts or totals tend to increase along with the number of individuals in the populations. Thus, proportions or means result in better comparisons because they include the population size as their denominator.

[^totals]: Of course, there do exist cases in which a count or total is more relevant for a specific problem than the corresponding proportion or mean. For instance, if you are in charge of allocating resources to the waste disposal facilities in your country, you care very much about what the total waste produced next year in each jurisdiction is projected to be and care only indirectly about what the mean waste produced per household in each jurisdiction is projected to be.

This is why statistics per capita are preferable to raw counts or totals for social data. For example, the statistician Thomas Lumley has a series of articles in which he expounds on the fallacy of using raw counts or totals when comparing statistics about Auckland, New Zealand, where he lives, to Wellington, New Zealand, all humorously titled something to the effect of "Auckland is larger than Wellington."

In one article, he comments on a news story that reported Auckland had 1,553 convictions of teenagers for drunk driving while Wellington only had 728. [@lumley_auckland_2012] In another article, he comments on news stories that reported about three times as many burglaries had occurred in Auckland as in Wellington. [@lumley_newsflash_2012] These counts are misleading because the population of Auckland is about three times that of Wellington.

What are more relevant for comparison between the two places are rate of teenage drunk driving convictions and rate of burglary _per capita._[^rate] As it turns out, after dividing by the missing denominator of population size, the rate of teenage drunk driving convictions is much lower in Auckland than in Wellington, but the rate of burglary is still slightly higher in Auckland than in Wellington.

[^rate]: Rate can be thought of as a proportion with the number of people experiencing an event in the numerator and the number of people in the population as the denominator.

Whenever you are presented with a count or total that you need to compare with another quantity in order to determine effect size, you may be able to locate the missing denominator. For instance, within a few minutes, Thomas Lumley was able to find the census figures for the number of teenagers in Auckland and in Wellington from a government website. [@lumley_auckland_2012] Once the relevant population size is identified, conversion from count or total to mean or proportion is a simple operation of division.

However, you might also encounter cases in which population size is not easily obtained information. In these cases, it is best to withhold judgment about practical significance.


#### Categorical Explanatory Variable

As previously mentioned, when the explanatory variable is categorical, you are essentially comparing the response variable between two or more populations, one for each level of the explanatory variable.


##### Numerical Response Variable

With a numerical response variable, the most common way to report effect size is in terms of the means of the response variable for each such population. The effect size is summarized as the difference in these means. For example, the mean expenditure on food and beverages for off-premises consumption in Michigan in 2020 was \$3,532, whereas it was \$2,872 in Arkansas in 2020. The effect size of living in Michigan versus living in Arkansas can be summarized as the difference in means of \$3,532 &minus; \$2,872 = \$660.

In these cases, you should search for both the mean response variable values reported individually and for the differences in means.

It is good to know the mean values themselves and not just the difference in means because the mean values can shed light on the relative magnitude of the difference. If the difference in mean values of weight between males and females of some species of animal is 2 pounds, the practical significance of this difference is not the same when the mean male weight is 48 pounds and mean female weight 41 pounds versus when the mean male weight is 589 pounds and mean female weight 413 pounds.

Furthermore, quality statistical analysis reports the difference in means separately from the individual mean values because estimation error should be calculated differently for the difference in the means than for the means themselves.

If neither the individual means nor the differences in means are reported, then you must withhold judgment as to practical significance, and you should question the veracity of the source.

Sometimes, other statistics are reported instead of the means of response variable. For various technical reasons, medians might be reported instead of means.[^median] However, your scrutinization of medians is similar to that of means. You should look for both the individual median values of each population and their differences, prefer to have both, but demand at least one of these.

[^median]: Medians are more resistant to outliers, so they are often used for variables with heavily skewed distributions, such as personal monetary income. Furthermore, analyses that use logarithm transforms are more easily interpreted in terms of medians than means.

Furthermore, sometimes _standardized statistics_ are reported instead. Mean or median values of the response variable and their differences are _unstandarized statistics:_ they are reported in the original units of measurement of the observations and relate back to the real world quantities on which they are based. A popular standardized statistic for this case results from dividing the difference in means of the response variable by the standard deviation of the response variable &ndash; commonly called "Cohen's _d"_ after the psychologist who popularized the practice.

Standardized statistics for effect size have their uses.[^standardized] However, whenever you can, you are better off consulting unstandardized statistics since they are more readily related to the real world and thus to subject-matter knowledge, which is the ultimate origin of practical significance. Thus, you should welcome standardized statistics of effect size as additional pieces of information, but still look for the unstandardized statistics, such as mean or median values of the response variable and their differences, and be skeptical when the unstandardized statistics are not provided.

[^standardized]: Papers by Nakagawa and Cuthill [-@nakagawa_effect_2007] and by Ferguson [-@ferguson_effect_2009] summarize these standardized statistics. Valid uses include comparing effect sizes when statistics are on different scales or when dealing with inherently abstract and unitless quantities such as those used by psychologists in personality tests.

Finally, note that both means and medians are measures of _just_ the central tendency of the distribution of a variable. The central tendency of a variable is only one way in which the variable's distribution can differ. Therefore, the difference in means or medians is only an initial summary of effect size, not a complete comparison of how the response variable differs in different populations.[^cohen] Such complete comparison is beyond the scope of this article and is addressed elsewhere.

[^cohen]: Some standardized statistics such as Cohen's _d_ incorporate information about the dispersion of the distribution of the variable, e.g., Cohen's _d_ involves division by the standard deviation, which is a measure of dispersion. However, Cohen's _d_ does not report on other properties such as skewness or kurtosis.


##### Categorical Response Variable

When the response variable is categorical instead of numerical, but the explanatory variable is still categorical, you are still essentially comparing the response variable between two or more populations. However, the response variable is now expressed in terms of proportions instead of means. Therefore, in this case, everything discussed in the previous section should be reiterated, but with proportions instead of means and differences of proportions instead of differences of means.

However, the major change in the categorical response case is that differences of proportions are not as ubiquitous as statistics of effect size as differences of means are in the numerical response case. This section, therefore, describes some additional summary statistics.

On the one hand, there is nothing wrong with using a difference in proportion to describe an effect size. For example, in September 2021, 66.67% of flights of JetBlue Flight Number 2495 from Newark to Miami were delayed more than half an hour, whereas 56.67% of flights of JetBlue Flight Number 2695, also from Newark to Miami, were delayed more than half an hour. [@noauthor_september_nodate] The effect size can be summarized as a difference of 66.67 &minus; 56.67 = 10 percentage points.

However, for effect sizes such as this that compare two different rates, there are two other summary statistics that are quite common: _relative risk_ and _odds ratio._

Relative risk[^risk] is defined as the ratio of two proportions. For example, when choosing between Flight 2495 and Flight 2695, you are likely curious how much less likely you are to experience a delay in September 2021 taking Flight 2695 instead of Flight 2495. This can be summarized with the relative risk of 56.67% / 66.67% = 85%.

[^risk]: The use of the term "risk" in "relative risk" comes from the idea of risk of disease or other adverse outcome, because relative risk is frequently used in medical statistics.

While relative risk is a useful summary statistic for effect size with a categorical response variable, when you encounter a relative risk, you should also look for the original, absolute risks, for reasons similar to why it is important, in the numerical response  variable case, to look for the individual means themselves in addition to the differences in means. Relative risk is just that, _relative,_ and can make risks that are small and risks that are large seem equivalent.

For instance, if the proportion of delayed flights for Flight 2495 was only 11.11%, and for Flight 2695 only 9.44%, the relative risk of experiencing a delay on Flight 2695 instead of Flight 2495 would again be 85%. However, the absolute risk of experiencing a delay on either flight is much smaller in this case, so should probably weigh less in your decision making.

The odds of an event is defined as the probability of an event occurring divided by the probability of the event not occurring. A proportion representing a risk can be expressed as an odds by dividing it by the quantity one minus itself. The odds of experiencing a delayed flight on Flight 2495 would be 66.67% / (1 &minus; 66.67%) = 2 to 1, and the odds of experience a delayed flight on Flight 2695 would be 56.67% / (1 &minus; 56.67%) = 1.31 to 1.

An odds ratio is the odds of one event divided by the odds of another event. The odds ratio for for experiencing a delay on Flight 2695 compared with Flight 2495 in September 2021 is 1.31 / 2 = 0.655.

This value of 0.655 might not seem very enlightening. Indeed, for moderate proportions such as 66.67% and 56.67%, an odds ratio is best interpreted only relatively to other odds ratios. The reason that odds ratios come up is that in retrospective or "case-control" studies,[^retrospective] relative risks cannot be directly calculated, but odds ratios can, and when the risks are very small (or very large), odds ratios _approximate_ relative risks. For instance, when Flight 2495 has 11.11% of its flights delayed and Flight 2695 has 9.44% of its flights delayed, the odds ratio is 0.834, which is close to the 0.85 relative risk.

[^retrospective]: Retrospective or "case-control" studies select individuals for the study based on the response variable, and then look for how the explanatory variable differs between response variable levels. This is opposite of how a randomized, controlled experiment works. For instance, in a medical context, individuals in a randomized, controlled experiment are assigned to a level of the explanatory variable (e.g., therapy or control), and then are measured in terms of the response variable, such as disease outcome. In a case-control study, the disease outcome is used for selection to the study, and then explanatory variables such as risk factors are measured.


#### Correlation

When the explanatory variable is numerical instead of categorical, effect size can again be presented in either unstandardized form or standardized form. Again, unstandardized statistics of effect size are preferable because they can more easily be related to subject-matter knowledge. However, with numerical explanatory variables, the most direct way to summarize an effect size using an unstandardized statistic involves model-based analysis, which is a more involved topic described in the next section.

This section describes standardized statistics of effect size for numerical explanatory variables. These are typically a correlation coefficient. There are too many kinds of correlation coefficients to summarize them exhaustively here, but there is one family of correlation coefficient that are used very commonly and summarized in this section.

This family includes the Pearson product-moment correlation coefficient, the Spearman rank correlation coefficient, and related statistics such as the point-biserial correlation coefficient. These correlation coefficients are so common that if a value of _r_ or _&rho;_ is presented as a correlation statistic without any other qualification, it is likely that one of these correlation coefficients has been used.[^greek]

[^greek]: The character _"&rho;"_ is the Greek letter rho. Sometimes a convention is used where _r_ refers to the Pearson product-moment correlation coefficient and _&rho;_ is used for the Spearman rank correlation coefficient. However, you should not rely on this convention, because there exists another convention in which Roman letters, such as _r,_ are used for statistics pertaining to a sample and Greek letters, such as _&rho;,_ are used for parameters pertaining to a population.

These correlation coefficients range from a value of -1 to a value of 1. A value of 1 represents an exact correlation, a value of 0 represents no correlation, and a value of -1 represents an exact inverse correlation.

```{r, eval=FALSE, include=FALSE}
iris
cor(iris$Sepal.Length, iris$Sepal.Width)
cor(iris$Petal.Length, iris$Petal.Width)
```

The Pearson product-moment correlation coefficient measures the linear association of two numerical variables. For instance, a famous data set in statistics contains lengths and widths of petals taken from three species of iris flowers. Petals that are longer also tend to be wider, and vice versa, such that the Pearson correlation coefficient for petal length and petal width in this data set is _r_ = 0.96, which is close to 1 and thus a strong correlation.

If every petal in the sample were the same exact shape, but only scaled up or scaled down without changing relative proportions of length and width, then the Pearson coefficient would be _r_ = 1.  Similarly, values of the Pearson correlation coefficient close to -1 indicate that the larger one variable gets, the more likely the other variable is to be smaller. A Pearson correlation coefficient close to 0 indicates that there is no linear association between two variables.

One of the major shortcomings of the Pearson product-moment correlation coefficient is that it only reflects _linear_ correlation between two variables. A second major shortcoming is that the Pearson correlation coefficient is not robust against outliers, such that even if there is a strong correlation among the majority of the observations, if there are just a relatively few observations that are outside this trend, then the Pearson correlation coefficient will be disproportionately closer to 0.

The Spearman rank correlation coefficient addresses these shortcomings by first converting values to their ranks and then performing a similar calculation to the Pearson correlation coefficient.[^ordinal] It is also interpreted on the same scale of -1 to 0 to 1. However, the Spearman rank correlation coefficient only captures monotonic relationships.

In a monotonic association, one variable always increases or always decreases as the other variable increases over the entirety of the variables' ranges. If a correlation involves a variable increasing as the other variable increases over part of the other variable's range, but decreasing as the other variable increases over another part, the Spearman correlation coefficient will be closer to 0.

[^ordinal]: Because the Spearman correlation coefficient uses ranks, it is also more appropriate for ordinal variables. Ordinal variables are those that have discrete levels, but also have some sense of order, such as _small, medium,_ and _large._

In cases in which the response variable is categorical and the explanatory variable is numerical, there are versions of correlation coefficients similar to the Pearson product-moment correlation coefficient. For instance, the point-biserial correlation coefficient can be used when a response variable has two possible levels and the explanatory variable is numerical. It is also interpreted on the same -1 to 0 to 1 scale.

Whichever correlation coefficient you encounter, you should understand what the correlation coefficient does and does not reflect and what the scale used for its values indicates.


#### Model-based Estimation

* Single variable
    * in original, unstandardized units, interpreting $\hat{\beta}$
    * example data set with simple linear regression fit
    * transformations
* Multiple variables
    * Interactions
    * Adjustment for confounders


### Confounding

* Confounding: Washington Post fact checker and the Obama gender pay gap statistic gaffe


## Estimation Error and Confidence Intervals


## Common Flaws of Research

* Statistical theater: _p_-values without inference
* Reporting _p_-value instead of effect size
* Lack of adjustment for multiple comparisons and _p_-hacking


_**Go back and rewrite to use second-person more and passive voice less.**_

_**Go back and replace any contrived examples with real world statistics.**_

_**Add more images.**_


## Citations

::: {#refs}
:::


## Footnotes
